## 阻塞式IO和非阻塞式IO
阻塞式io是当io没准备好时 进程进入阻塞状态。
网络上很多文章，包括一些书籍都说阻塞IO的性能差，但事实上并不能说阻塞IO的性能差，只能说阻塞IO的开销较大。
### 单进程下
在网络编程中，一般服务器需要处理多客户端，如果是像这种单进程服务器。与一个客户端连接建立之后，服务器就会使用read等系统调用对客户端进行数据读取来处理请求。当该客户端没有发送数据或者发送的数据还没有达到时，服务器就会进入阻塞状态，此时整个服务器进程就会挂起。当其他客户端连接请求达到时，服务器由于处于挂起状态，什么也不能做，所以也不能对其他客户端进行处理。因此，这种单进程的阻塞式IO的服务器只能处理一个客户端的情况，这样的服务器没有任何的实用性
### 多进程下
阻塞会使线程有运行态转变为就绪态，那么就对应一次进程的上下文，进程上下文切换会切换虚拟内存，栈，全局变量等资源，需要花费比较多的时间，而且等待的事件发生之后，就还会从就绪态转变为运行态，这里有涉及到进程的上下文切换，那么这样一次同步的阻塞IO就会导致两次的进程上下文切换。所以说阻塞IO的开销比较大。 

### 非阻塞式IO
开销大 实际上是CPU资源的浪费 
与多路复用IO的区别是 多路复用IO是将轮询操作交给操作系统的内核态 减小了切换的开销
## IO多路复用
### select
单个进程select监听的fd数量有限 不超过1024；
有fd数组在用户态和内核态之间复制
采用轮询方式 随着fd的增加 轮询的复杂度为线性增加

### poll
没有fd个数的限制；
有fd数组在用户态和内核态之间复制
同样要轮询所有fd

### epoll
没有描述符的限制；
使用mmap内存映射加速内核和用户空间的数据传输
不需要轮询所有fd，通过就绪队列来存储就绪fd;
两种触发方式：电平触发（select/poll） 和边沿触发

epoll_create 创建epoll描述符 ,epoll_ctl 注册\清除一个fd到epoll,epoll_wait 等待轮询结果 支持超时设置

# 进程通信的方式
1. pipe 管道是单项的 开两个fd fd[0]负责读 fd[1]负责写

2. socketpair 双向的

3. eventfd 缓冲区管理简单 全部"buffer"只有定长8bytes
上述方式都可以产生fd 通过poll或epoll来管理

## linux网络头文件
sys/types.h：数据类型定义

sys/socket.h：提供socket函数及数据结构

netinet/in.h：定义数据结构sockaddr_in

arpa/inet.h：提供IP地址转换函数

netdb.h：提供设置及获取域名的函数

sys/ioctl.h：提供对I/O控制的函数

sys/poll.h：提供socket等待测试机制的函数


其他在网络程序中常见的头文件
unistd.h：提供通用的文件、目录、程序及进程操作的函数

errno.h：提供错误号errno的定义，用于错误处理

fcntl.h：提供对文件控制的函数

time.h：提供有关时间的函数

crypt.h：提供使用DES加密算法的加密函数

pwd.h：提供对/etc/passwd文件访问的函数

shadow.h：提供对/etc/shadow文件访问的函数

pthread.h：提供多线程操作的函数

signal.h：提供对信号操作的函数

sys/wait.h、sys/ipc.h、sys/shm.h：提供进程等待、进程间通讯（IPC）及共享内存的函数

## 为什么需要有应用层缓冲区？
https://www.cnblogs.com/fortunely/p/16136071.html

non-blocking网络编程中，non-blocking IO核心思想是避免阻塞在read()/write()或其他IO系统调用上，可以最大限度复用thread-of-control，让一个线程能服务于多个socket连接。而IO线程只能阻塞在IO-multiplexing函数上，如select()/poll()/epoll_wait()，这样应用层的缓冲区就是必须的，每个TCP socket都要有stateful的input buffer和output buffer。
具体来说
* TcpConnection必须要有output buffer
  一个常见场景：程序想通过TCP连接发送100K byte数据，但在write()调用中，OS只接收80K（受TCP通告窗口advertised window的控制），而程序又不能原地阻塞等待，事实上也不知道要等多久。程序应该尽快交出控制器，返回到event loop。此时，剩余20K数据怎么办？
  对应用程序，它只管生成数据，不应该关系到底数据是一次发送，还是分几次发送，这些应该由网络库操心，程序只需要调用TcpConnection::send()就行。网络库应该接管剩余的20K数据，把它保存到TcpConnection的output buffer，然后注册POLLOUT事件，一旦socket变得可写就立刻发送数据。当然，第二次不一定能完全写入20K，如果有剩余，网络库应该继续关注POLLOUT事件；如果写完20K，网络库应该停止关注POLLOUT，以免造成busy loop。

如果程序又写入50K，而此时output buffer里还有待发20K数据，那么网络库不应该直接调用write()，而应该把这50K数据append到那20K数据之后，等socket变得可写时再一并写入。

如果output buffer里还有待发送数据，而程序又想关闭连接，但对程序而言，调用TcpConnection::send()后就认为数据迟早会发出去，此时网络库不应该直接关闭连接，而要等数据发送完毕。因为此时数据可能还在内核缓冲区中，并没有通过网卡成功发送给接收方。
将数据append到buffer，甚至write进内核，都不代表数据成功发送给对端。

综上，要让程序在write操作上不阻塞，网络库必须给每个tcp connection配置output buffer。


* TcpConnection必须要有input buffer
  TCP是一个无边界的字节流协议，接收方必须要处理“收到的数据尚不构成一条完整的消息”“一次收到两条消息的数据”等情况。
一个常见场景：发送方send 2条10K byte消息（共计20K），接收方收到数据的可能情况：
一次性收到20K数据
分2次收到，第一次5K，第二次15K
分2次收到，第一次15K，第二次5K
分2次收到，第一次10K，第二次10K
分3次收到，第一次6K，第二次8K，第三次6K
其他任何可能
这些情况统称为粘包问题。
LT模式下，如何解决的粘包问题？
可以一次把内核缓冲区中的数据读完，存至input buffer，通知应用程序，进行onMessage(Buffer* buffer)回调。在onMessage回调中，应用层协议判定是否是一个完整的包，如果不是一条完整的消息，不会取走数据，也不会进行相应的处理；如果是一条完整的消息，将取走这条消息，并进行相应的处理。

如何判断是一条完整的消息？
相应应用层协议制定协议，不由网络库负责。

网络库如何处理 “socket可读”事件？
LT模式  必须一次性把socket中数据读完，从内核缓冲区读取到应用层buffer，否则会反复触发POLLIN事件，造成busy-loop。通知上层应用程序，On Message回调，根据应用层协议判定是否是一个完整的包,解码，如果不是一条完整的消息 不会取走消息 也不会进行进行相应处理。如果是一条完整消息，将取走这条消息 并进行相应处理。

综上，tcp网络编程中，网络库必须给每个tcp connection配置input buffer。

应用层与input buffer、output buffer

muduo中的IO都是带缓冲的IO（buffered IO），应用层不会自行去read()或write()某个socket，只会操作TcpConnection的input buffer和output buffer。更准确来说，是在onMessage()回调中读取input buffer；调用TcpConnection::send() 来间接操作output buffer，而不直接操作output buffer。

## muduo Epoll是LT模式 
原因 
1. 与poll兼容
2. LT模式不会发生漏掉事件的bug 。
3. 读写的时候不需要等候EAGAIN.节省系统调用的次数，降低延迟（如果用ET 读的时候必须读到EAGAIN 因为边缘触发必须循环读读到egaing否则会漏事件； 写的时候必须写到output buff写完 或者EAGAIN 原因同理 ）

注意：LT模式下不能一开始就关注POLLOUT写事件，否则会出现busy loop。应该在write无法完全写入内核缓冲区的时候开始关注POLLOUT，并将未写入内核缓冲区的数据添加到应用层output buff,知道output buf数据写完，停止关注pullout事件。为了提高效率，muduo的LT一旦触发读事件会分配大缓冲区尽可能将数据读完

## Acceptor类和TcpConnection类
TcpConnection类和Acceptor类是兄弟关系，Acceptor用于main EventLoop中，对服务器监听套接字fd及其相关方法进行封装（监听、接受连接、分发连接给SubEventLoop等），TcpConnection用于SubEventLoop中，对连接套接字fd及其相关方法进行封装（读消息事件、发送消息事件、连接关闭事件、错误事件等）


## TcpConnection类的生命周期
1. 建立之初保存在TcpServer的connections_中，这个connections_是一个map，键是字符串类型，值就是shared_ptr<TcpConnection>类型，所以建立之初，TcpConnection对象的引用计数为1
2. 创建TcpConnection后，TcpConnection为内部用于监听文件描述符sockfd的Channel传递保存着自己指针的shared_ptr，但是Channel中的智能指针是以weak_ptr的形式存在的（tie_），不增加引用计数，所以此时仍然是1
3. 在TcpConnection处于连接创建的过程中未有shared_ptr的创建和销毁，所以仍然是1
4. 客户端主动关闭连接后，服务器端的TcpConnection在handleClose函数中又创建了一个shared_ptr引用自身的局部变量，此时引用计数加一，变为2
5. 紧接着调用TcpServer::removeConnectionInLoop函数，因为传入的参数是引用类型，不存在赋值，所以引用计数仍为2
6. 在removeConnectionInLoop函数中，TcpServer将TcpConnection从自己的connections_（保存所有tcp连接的map）中删除，此时引用计数减一，变为1
7. TcpServer通过std::bind绑定函数指针，将TcpConnection::connectDestroyed函数和TcpConnection对象绑定并放到EventLoop中等待调用，因为std::bind只能是赋值操作，所以引用计数加一，变为2
8. 返回到handleClose函数中，函数返回，局部变量销毁，引用计数减一，变为1
9. EventLoop从poll中返回，执行TcpConnection::connectDestroyed函数，做最后的清理工作，函数返回，绑定到这个函数上的TcpConnection对象指针也跟着销毁，引用计数减一，变为0
10. 开始调用TcpConnection析构函数，TcpConnection销毁

所以如果在第7步不使用std::bind增加TcpConnection生命期的话，TcpConnection可能在handleClose函数返回后就销毁了，根本不能执行TcpConnection::connectDestroyed函数


## Muduo中网络服务器的关键步骤发生的时间
1. int socket (int __domain, int __type, int __protocol);指定地址族规范，类型规范和服务属性和具体协议类型。返回值是新创建的sock_fd。返回-1代表失败。
   
发生在Acceptor类的构造函数中，它的上文是**TcpServer类的构造函数**(Acceptor类是TcpServer类的成员对象);Acceptor将调用socket函数并将得到的sock fd作为参数传给Socket类对象。（Acceptor类包含一个Socket类对象 构造Acceptor类时会先构造其成员类Socket，而这时调用的socket得到了fd后作为Socket构造函数的实参）

2. int bind(sockfd, (struct sockaddr*)&my_addr, sizeof(my_addr))。参数是待绑定的fd和待绑定的socket地址。成功是返回0，失败返回-1并设置errno。两种常见的errno是**EACCES**:表示绑定的地址是受保护的地址，仅超级用户能访问，如普通用户将socket_fd绑定到端口号包含（1-1023）的地址。**EADDRINUSE**：表示当前被绑定的地址正在使用

发生在Acceptor类的构造函数中;Acceptor于先前创造了一个socket对象后，随即调用socket对象的bindAddress()函数绑定端口 并设置scokfd的相关属性比如ReuseAddr\ReusePort等

3. int linsten(int sockfd,int backlog):参数是监听的fd和内核监听队列的最大长度。成功时返回-，失败返回-1。系统处于ESTABLISHED状态的连接数量收backlog数量决定。

发生在TcpServer::start()函数中：在这个函数中会将Acceptor::listen函数bind到loop_的runInLoop中执行。Acceptor::listen函数里会调用其Socket成员的Linsten函数。TcpServer::start()只会执行一次。

4. int accept(int sockfd,struct sockaddr* peerAddr ,socklen_t* addrlen);参数是接收的sockfd,以及一个置为空的用来保存接收到的客户端socket地址的结构体和结构体大小。成功时返回一个新的conn_fd用于客户端和服务端通信 失败时返回-1

发生在Acceptor::handleRead()函数中，这个函数的上文是Accepot类中的Channel对象触发可读事件，当可读事件发生时Channel会回调handleRead函数。(具体是EventLoop的loop()函数触发event事件到来，调用对应channel的 Channel::handleEvent函数)


## TcpConnection对象引用计数变化
1. TcpServer::newConnnection()中 第一次创建conn 引用计数 = 1
2. TcpServer::newConnnection()中 将创建的conn加入到了ConnectionMap中，引用计数+1 = 2
3. TcpServer::newConnnection() 最后调用了TcpConnection::connectEstablished() 在这里TcpConnection对象将两次创建它自身的临时对象shared_from_this()，一次是给channel_的tie()创建一个弱引用;另一次是给TcpConnection的ConnectionCallback函数。临时对象会+1但又马上-1
4. 经过3执行完connectEstablished 回到 TcpServer::newConnnection()中。 此时TcpServer::newConnnection()执行完毕 创建的conn智能指针被销毁 ,此时只剩下Map中的引用计数 = 1
5. 当此Connnection有事件到来时，在Eventloop中通知它的channel，它的channel会调用handleEvent处理事件。此时在Channel::handleEvent创立 guard 临时对象，gurad会将第3步中的弱引用尝试提权为share_ptr()，提权成功的话引用计数+1 = 2；handleEvent()函数执行结束引用计数会-1 = 1
那在整个连接阶段conn的引用计数都一直是在1-2之间浮动
6. Channel::handleEvent处理的可读事件调用readCallback_(),实际上调用的是TcpConnection::handleRead()或者Acceptor::handleRead();若是TcpConnection::handleRead()且在过程中read()返回0，说明客户端关闭连接。此时调用TcpConnection::handleClose(). 第6步中目前没有引用计数变化
7. 在TcpConnection::handleClose()中，会生成TcpConnectionPtr临时对象  引用计数+1 =3(在第5步基础上，此时程序还在channel的handleEvent()事件中)。然后回通过这个回调这个临时对象的closeCallback_,即TcpServer::removeConnection。
8. 在TcpServer::removeConnection中回移除在TcpServer的TcpServer中的该conn对象，即第2步创建的对象 引用计数-1 = 2.并回调TcpConnection::connectDestroyed()，将该conn的channel_取消所有事件关注并从loop中移除，并将conn状态设为kDisconnected。 调用connectionCallback_显示一次“conn down”。
9. 经过8回到7后TcpConnection::handleClose()会结束，7中产生的TcpConnectionPtr临时对象会被析构，引用计数 = 2 -1 =1。此时只剩5Channel::handleEvent中产生的guard临时对象。待handleEvent()结束 该conn对象会被最终析构

## TcpConnection类为什么要继承enable_shared_from_this类
因为用裸指针类型构造share_ptr<T>对象时，会重新构造一个新智能指针对象 其引用计数只能为1。只有 **智能指针**的**拷贝构造函数** 和**赋值构造函数**才会使引用计数+1
```c++
int a = 2;
int* ptr = &a;
share_ptr<int> sp1(ptr); //sp1和sp2的引用计数都是1
share_ptr<int> sp2(ptr); //

share_ptr<int> sp3(sp2) //用智能指针作为参数构造才会改变引用计数 sp2,sp3的引用计数都为2
```

## muduo的系统套接字使用
0 标准输入,1 标准输出,2 标准错误
3 pollerFd(EventLoop中) /*3-5的fd会随着EventLoop对象的增加而翻倍**/
4 timerFd(EventLoop中)
5 wakeupFd(EventLoop中)

6 sockFd(Acceptor中)
7 idleFd(Acceptor中)
8 connFd(来一个连接就会就会新分配一个Fd)

## 服务器端调用shutdonw()主动断开连接
客户端会read() ==0,然后在客户端会close(connfd);
然后服务端会read到POLLHUP和POLLIN,此时server会read() ==0 从而调用close释放资源

POLLHUP - 挂断（仅输出）
POLLNVAL - 无效请求：fd 未打开（仅输出）

## std::any 绑定一个未知类型的上下文对象
相比void*类型: void* 不是类型安全的
std::any时任意类型的类型安全存储和安全取回